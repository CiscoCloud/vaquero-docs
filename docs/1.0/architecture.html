<head>
            <meta charset="UTF-8">
            <!--[if IE]><meta http-equiv="X-UA-Compatible" content="IE=edge"><![endif]-->
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>Vaquero Documentation</title>
            <link rel="stylesheet" type="text/css" href="../doc.css">
            <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400">
            <style>
                .markdown-body {
                    box-sizing: border-box;
                    min-width: 200px;
                    max-width: 980px;
                    margin: 0 auto;
                    padding: 45px;
                }
            </style>
        </head><article class="markdown-body"><h1>Vaquero</h1>

<p>The Vaquero project is designed to simplify the provisioning and ongoing operations of clustered software on bare metal infrastructure. A running system will be composed of a centralized control plane that automates provisioning of software in one or more datacenters.</p>

<p>The goal is to provide the ability for teams to manage their infrastructure using the same tools they use for their applications (revision control, CI/CD pipeline, etc.), and enable similar workflows including automated updates, gating, immutability, A/B deployments, etc. The final outcome will be a fully operational datacenter running heterogeneous deployments of clustered software on bare metal with fully automated deployment and upgrades driven by a CI pipeline.</p>

<h2>Architecture</h2>

<p>The diagram linked below shows a high-level view of the overall application architecture. All components should run as containers, but some will need various levels of privilege to perform their required functionality.</p>

<p><img src="https://raw.githubusercontent.com/CiscoCloud/vaquero-docs/gh-pages/docs/current/architecturediagram.png" alt="arch"></p>

<h3>The Datacenter Node</h3>

<p>Each datacenter will be able to operate without an active connection to the master nodes once the active datacenter configuration has been staged to those nodes. The system is composed of multiple services which should be operated with redundancies for availability.</p>

<h4><code class="prettyprint">vaquero agent</code></h4>

<p>The <code class="prettyprint">vaquero</code> process in <code class="prettyprint">agent</code> mode registers itself with an upstream master and drives provisioning in a local datacenter.</p>

<ol>
<li><strong>PXE Boot Service</strong> -- implements the necessary protocols to get a node from PXE ROM to a working Linux kernel.</li>
<li><strong>Lifecycle Service</strong> -- implements the necessary protocols to manage the lifecycle of a server. This can leverage O/S-based mechanisms to hardware-based systems driven by IPMI, etc.</li>
<li><strong>State Engine</strong> -- implements a state tracking system to manage multi-step configurations, and provide status updates to master nodes.</li>
</ol>

<p>The agent will have a built-in HTTP server that will be used for API endpoints, as well as asset delivery. The assets delivered can be provided through multiple mechanisms based on configuration, including potentially being cached (uploaded) locally, or passed through from other systems (i.e. CDN).</p>

<p>Some endpoints will be passed through directly to internal services, but others will be terminated by the agent (more details in [API documentation]):</p>

<ul>
<li><strong>/assets</strong> - static assets required to boot client nodes.</li>
<li><strong>/state</strong> - mechanism for client nodes to report state (via API).</li>
<li><strong>/status</strong> - used to inspect current operational state of this agent, and its client nodes.</li>
</ul>

<p>CoreOS Bare Metal has the following endpoints documented:</p>

<ul>
<li>[HTTP API]</li>
<li>[gRPC API]</li>
</ul>

<h3>The Control Node</h3>

<p>The overall solution will be driven by a centralized control system node that manage the process of transforming updates for our Source of Truth (SoT) into configurations that can be applied by the datacenter nodes. To perform this transformation, the system will need to process updates from the SoT, compile the changes, and stage those changes for implementation.</p>

<p>The structure of this data is defined [elsewhere].</p>

<h4><code class="prettyprint">vaquero server</code></h4>

<p>The <code class="prettyprint">vaquero</code> application in <code class="prettyprint">server</code> implements a simple HTTP-based API that manages the overall workflow. To this end a few endpoints have been planned (these will be further details in the [API documentation]):</p>

<ul>
<li><strong>/postreceive</strong> - accepts inbound webhooks indicated an update has occurred in the SoT.</li>
<li><strong>/status</strong> - used to inspect current operational state of the Vaquero system, and agent nodes.</li>
<li><strong>/prepare</strong> - tells Vaquero system to stage a configuration.</li>
<li><strong>/execute</strong> - performs the machine provisioning through lifecycle management, etc.</li>
</ul>

<h3>State Management / Coordination</h3>

<p>There is a need in Vaquero to coordinate work between multiple instances in a reliable way. While no design has been completed around the structure of this data, we anticipate that a strongly consistent data store, such as <code class="prettyprint">consul</code> or <code class="prettyprint">etcd</code>, will be necessary.</p>

<h2>Deployment and Availability Considerations</h2>

<p>As mentioned above, Vaquero will be delivered as several containerized services. To execute the software will require an environment that can run each of these services, and expose various ports to the network. (needs doc)</p>

<p>No explicit runtime environment is required, aside from a recent Docker engine, and sufficient capacity to run the containers required.</p>

<p>To achieve a highly-available system, an operator should plan to run redundant instances of the various services, and ensure they&#39;re properly connected to the coordination system. (needs docs)</p>

<p>Finally, the control node can also act as an agent if that is desired. This can be useful to allow the control node to bootstrap agent nodes directly, or if a multi-datacenter deployment is not required.</p>
